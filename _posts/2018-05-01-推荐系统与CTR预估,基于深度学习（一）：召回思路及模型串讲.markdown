---
layout:     post
title: 推荐系统与CTR预估， 基于深度学习（一):召回思路及模型串讲
date:       2018-05-01 12:01:00
author:     "nickiwei"
header-img: "img/post-bg-2015.jpg"
tags:
    - 深度学习
    - DSSM
---

这个系列介绍了在深度学习的大背景下， 推荐系统领域所发生的翻天覆地的变化。 从召回和排序模型串讲开始， 结合论文总结与工程实践， 并充分记录了作者在实际工作中所总结的各种经验总结。

欢迎转载， 转载请注明出处及链接。

# 基于深度学习的推荐系统基本架构
 一个推荐系统的架构其实并不复杂， 基本上就是召回， 排序， 规则三个部分。

![Architecture](/img/RC01.jpg)

召回模型负责根据用户和场景特征， 从众多的内容通道中抓取用户可能感兴趣的内容， 召回模型可能是多通道并行的， 通道与通道之间互不影响。召回模型也叫做触发模型。

排序模型根据CTR等指标， 对来自不同通道的所有召回内容进行排序。 在Feeds中， 内容按照排序模型的输出自上而下显示， 在广告推荐中， 选择CTR最高的若干个进行展示。除了合并通道的原因以外， 召回和排序两个阶段所使用的特征也大不相同， 这样的做法可以避免单模型特征爆炸。

规则部分则是对模型的补充。在实践中主要起到两个方面的作用， 

* 处理系统对于多样性， 实效性等的个性化需求。
* 应对一些特殊的， 紧急的需求， 做hard code.

这个系列中， 我们将依序介绍这三个部分， 本文重点介绍召回模型。我们将着重记述两大类召回模型， 分别是微软自2013年起发表的DSSM模型系列， 以及Youtube在2017发布的Youtube DNN模型。

# DSSM

DSSM是微软2013年发表的一个query/doc相似度计算模型， 后来被发展成一种框架广泛的应用在各种召回， 排序问题中。

## Original DSSM

我们首先来看一下基本的DSSM模型：

![DSSM](/img/ZH01.png)

模型很容易理解， 首先， 我们将Query和Doc词向量（one hot）转化为embedding向量， 这里， 针对英文输入， 原论文中提出了一种特殊的embedding方法， 用来显著降低embedding字典规模， 称为word hashing， 但是中文中这种方式不是很适用， 我们在中文embedding时， 使用传统的方式进行embedding即可（word hashing 方法会在下一部分详细介绍）


经过embedding后的词向量， 会喂给DNN进行多层projection， 与绝大多数nlp问题一致， 我们选择tanh激活函数。 之后， Query和Doc向量会进行一次cos相似度计算， 得到R(Q, Dn), 各个R(Q, Dn)通过softmax归一化后得到最终的指标P(Dn\|Q).

公式如下：

![DSSM](/img/ZH03.jpg)

![DSSM](/img/ZH04.png)

在实际的训练中， 我们提出一种类似word2vec negative sampling的负采样方式进行训练。在每一次迭代中， 我们都选取一个正样本和四个负样本， 最后的loss计算中， 我们取负采样交叉商， P(D+\|Q)取正loss， P(D-\|Q)取负loss， 如下图:

![DSSM](/img/ZH07.jpg)

## DSSM 的衍化

DSSM的key idea是：

### 将不同的对象映射到同一个语义空间中， 利用该语义空间中的距离计算相似度。

这一思路被广泛的应用到了 召回， 排序， 搜索等各种工程实践中。 在应用过程中， 我们更多是使用一个如下图的DSSM的模子， 然后再框架内部根据实际任务填充不同的拓扑：

![DSSM](/img/ZH05.jpg)

如上图， 我们将不同对象通过embedding 映射到同一语义空间中， 经过随机填充的不同拓扑（如FCN， CNN， RNN， LTR等）， 最终形成一个等维向量， 在定义一个距离公式sim(X, Y), 求得不同对象在该语义空间中的距离。 通常情况下， 我们均使用cos距离即可。 所以模型的核心是根据不同的任务类型， 选择合适的f， g函数拓扑。 所谓的CNN-DSSM, LTR-DSSM都是这种框架的一个应用而已。

常见的DSSM变形应用有：

* 在CTR预估中，对点击与否做0，1二元分类，添加交叉熵损失变成一个分类模型

数据格式：
	
	- source word list   - target word list   - target
	苹果 六 袋    苹果 6s    0
	新手 汽车 驾驶    驾校 培训    1

* 在需要对一个子串打分时，可以使用余弦相似度来计算相似度，变成一个回归模型

数据格式：
	
	- source word list   - target word list   - target
	苹果 六 袋    苹果 6s    0.1
	新手 汽车 驾驶    驾校 培训    0.9

* 在排序学习中，在可变结构中添加 pairwise rank损失，变成一个排序模型（具体查看我的另一篇文章： 推荐系统与CTR预估（三）: LTR排序模型）

数据格式：

	   - source word list   - target1 word list   - target2 word list   - label
	   苹果 六 袋    苹果 6s    新手 汽车 驾驶    1
	   新手 汽车 驾驶    驾校 培训    苹果 6s    1


值得注意的是， 在排序模型中， 我们需要用到多个DSSM可变模型的组合。如下图：

![DSSM](/img/ZH06.jpg)

# MVDSSM 和 TDSSM

除了改动DSSM的可变结构外， 我们还提出了两种对DSSM模型的优化。这两个模型我们将在下一篇中详细介绍。

## MVDSSM

在前述的DSSM模型中， 我们采用双塔模型， 塔内部可根据需要替换成不同的拓扑。 训练时， 我们将一个正样本和四个负样本传入模型， 然后将softmax归一化后的P(D+\|Q)作为这一组训练样本的loss， log求和后取得最终的loss. 最终， 我们得到了一个关于Doc和Query之间的相关性计算模型. 

在推荐系统中， 我们通常将User看作一个Query(User的各种离散特征embedding后concat在一起构成其向量)， 将每一个item向量视作一个Doc(item的各种离散特征以相同的方式进行处理)。 但是， 通常情况下， 一个DNN只能处理一种类型的item（如网页的搜索记录， 应用下载记录， 电影观看记录等）， 原因在于， 这些不同的item录得了不同的离散特征， 因此， 构成的embedding向量也是不同的。 此时， 我们可以用多个DSSM模型分别训练， 从而完成对不同item的召回。但这样的模型最大的问题在于， 

* 我们得到了若干个互不相关的user模型， 这既浪费空间， 又影响性能。
* 不同view之间的数据存在的某种联系被我们完全忽略了。

因此， 一个自然的想法是， 打通双塔模型中的user塔(共享参数)， 构造一个单user多item view的多塔模型。 这就是MV-DSSM. 具体如下图:

![DSSM](/img/ZH09.jpg)

这个图乍一看跟DSSM很像， 但我们要注意到以下区别：

* DSSM中， 所有item样本（不论是正样本还是负样本）实际上共享一个塔。 而在MV-DSSM中， 这些item view塔是完全独立的。
* 在DSSM中， 所有的item采用相同的embedding， 在MV-DSSM中， embedding的方式可能是不一致的。

总体而言， MV-DSSM是DSSM基于user model共享参数的自然延伸， 二者之间联系远大于区别。特别是， 在DSSM和MV-DSSM中， 我们均可以自由选择FCN/CNN/RNN等不同的中间拓扑， 这一点是完全一致的。      

## MV-DSSM的训练

首先， 我们需要构造合适的数据集。 我们实际构造的向量是一个(user, item view)二元对，它包含一个用户点击的item view及其对应的user信息。 在训练时， 被选中的item view进行embedding， 其他item view全部用0向量填充。 我们希望优化的目标是， 所有P(Item|User)之和最小。其公式如下：

![DSSM](/img/ZH10.jpg)

值得注意的是， 理论上说我们可以选择任意的(user, item view)数据对作为训练样本， 但是为了简便， 我们通常选择同一个用户集的不同item view数据进行训练， 这样的好处在于：

* 一定程度上， 抑制user模型大幅震荡， 帮助user模型收敛。
* 便于进行feature normalization.

### 特征和数据选择

在特征和数据选择上， 论文中提出了多种特征降维的方法， 比如LSH(Local sensitive Hashing), Kmeans特征聚类， top1%特征抓取等， 在工程实践中， 由于我们在之前的LR系统中已经积累了相当的特征工程经验， 我们没有采用这些方法， 而是复用LR特征， 并通过单特征CTR评估法对大量特征进行了复评。

在数据选择上， 我们遵从了论文介绍的方法， 对用户集中的每一个user， 都逐一筛选了每一个view中的一组数据（实际上， 我们只训练了资讯和短视频两组view， 这是我们的业务所决定的）， 并搭配4组（随机选择的）负采样数据。这里， 论文中正负采用数据为1:9， 我们选择了1:4， 原因在于：

* 1:4的数据比例恰为DSSM时的训练数据比例， 我们可以服用该数据集。
* 同样比例的同样数据， 更有利于我们直接对比Single View 和Multi View DSSM的数据情况。

### 训练指标

最后， 我们仿照论文采用了以下两个重要指标作为评测标准：

### MRR(Mean Reciprocal Rank)

在1：4的训练集中， 模型按P(Item|User)将数据排序后， 正样本所在位置在全数据集中的平均值。

### P@1(Precision@1)

在1：4的训练集中， 模型按P(Item|User)将数据排序后， 正样本排在首位的比例。

这两个数据指标相互补充， MRR衡量模型排序的平均水平。 P@1衡量模型完全正确的优秀率。二者的trade-off在于：

我们既希望有更多的数据被正确的放在第一位， 同时也需要考虑， 即使不在第一位， 至少排在前三位的概率。

## MV-DSSM的优势

对比MV-DSSM和多个Single View DSSM组合， MV-DSSM在实践中的优势是显著的， 具体来说:

* 对比每一个Single View DSSM， MV-DSSM在该view上的数据都是占优的。
* 只有一个User Model， 既节省了空间， 同时， 也定义了一个更准确的User Model. （该User Model是否可以在其他场景复用， 是我们计划去测试的一个重要课题）
* 由于User Model共享参数， 我们可以快速的， 用较少的数据， 为模型添加一个新的view， 在scalability方面， MV-DSSM体现出惊人的优势。 

事实上， 这一点对我们来说是震惊的， 起初我们担心， 添加新的view并用不充足的数据去训练模型， 很可能导致模型不收敛 或者对其他view推荐的准确性产生影响。 但事实来看， 我们用很少的数据训练一个新的view， 模型不仅仅可以快速对新view产生一个较优秀的推荐（相比重新训练一个single view DSSM而言）（快速收敛）， 更重要的在于， 对于之前的view的推荐性能几乎没有影响。

这里也引入了另一个问题， 就是到底应该一个一个view单独训练， 还是将所有view混合训练。 从我们的测试情况来看， 在模型初期训练时， user model还不稳定， 这是最好是多个view的数据随机混合训练（增加user model对不同view数据的鲁棒性）。 当user model逐渐稳定后， 可以一个一个view单独添加， 并不会对模型产生较大影响。<i>（这个问题目前我们还研究的不是很透彻， 具体分析可能之后我会再重新深入研究）</i>

### 通过共享部分参数， 帮助新模型快速收敛。 同时， 增加共享部分对于复杂数据的鲁棒性。 

MV-DSSM的这种思想在很多任务中都有着特别的价值， 值得借鉴。

## TDSSM

MV-DSSM很好的解决了基于静态需求的召回问题。 下一个问题是， 如何处理用户短期的， 动态的需求推荐。 一个最基本的想法是， 只选择最近一段时间的用户数据进行训练， 这样有以下几个问题：

* 数据受限， 可能导致数据不充分。
* 需要每隔一段时间就重新训练模型， 这是非常昂贵的。
* 仍然不能很好的解决冷启动问题。

为此， 微软在2016年提出了TDSSM, 在DSSM的基础上， 通过加入对短期数据基于时间切片的RNN序列建模， 强化模型对短期兴趣的识别和推荐。 具体如下图：

![DSSM](/img/ZH11.jpg)

模型很好理解， 就是在User Model这边再加入一个Temporal Feature的小塔， 然后将两个user model输出的向量按如下的任意一种方式concat在一起， 其余均与DSSM保持一致。 

![DSSM](/img/ZH12.jpg)

在训练时， 也与DSSM保持一致， 我们选择一组(user, item)的正样本， 再随机生成4组(user, item)的负样本， 最终希望优化的目标， 是这几组样本R(Item, User)softmax归一化后的正样本概率P(Item+|User), 如下：

![DSSM](/img/ZH13.jpg)

## MR-TDSSM

在TDSSM中， 一个很重要的问题是时间片大小的选择。 太小，我们可能会得到非常稀疏的向量。 太大， 我们有很难捕获到用户的短期兴趣变化。因此，原论文中提出了一种MR-TDSSM， 基本上就是分别训练两个RNN， 应对较小（如一天）和较大（如一周）的时间切片数据。 最后将两个输出求和（这有就同时解决了稀疏性和波动性不足两个问题）再与静态用户向量concat在一起。

为了应对更大的模型，我们可以提前训练好DSSM的部分， 再进一步训练RNN的部分。 这样可以指数级降低训练复杂度。

## TDSSM的业务应用

目前， 在我们实际的模型中， 还没有开始测试相关TDSSM模型。原因是：

* 目前的资讯和视频系统对用户短期兴趣变化的捕捉需求不大。 
* 由于我们DSSM整体的召回漏出率只占20%, 大量的召回还是通过其他召回方式贡献， 比如最直接的热度召回。 因此， 短期兴趣变化 更多是依靠其他召回补充。 暂时看不到在DSSM中引入短期兴趣变化的需求。 

# Youtube DNN

除了DSSM外， Youtube在2017年提出了一套独立的模型拓扑设计， 取得了非常不错的效果。 由于我们线上目前阶段还是以DSSM为主， 因此， 这一部分我们只是简单的介绍一下大致的idea.

另外， Youtube的这篇文章， 有很多的工程经验也非常值得参考， 有兴趣的朋友可以自行详细阅读。

## 模型简介

![DSSM](/img/ZH14.jpg)

如上图， YoutubeDNN的思路可以说是传统协同过滤思路的之间衍化。 在传统的协调过滤中， 我们强调:

* 相似的用户 喜欢的物品 也相似。
* 喜欢A商品的用户， 可能也喜欢和A商品相似的其他商品。

这里，我们实际上是构造了两个向量空间， 用户空间和商品空间， 我们在任意一个向量空间中找到相似性， 再根据这种相似性进行推荐。

YoutubeDNN在此基础上， 做了一下两个重要的改变：

* 用一个统一的（用户， 商品）向量空间代替原来两个独立的向量空间。
* 用一个深度网络对该空间中的向量进行映射， 一方面是通过各种映射降低向量的维度， 减少最后一步求相似的计算量； 另一方面， 深度网络同时也是一个高阶特征提取的过程， 理论上， 我们应该能发现更高阶的用户相似性， 作出更准确的判断。

所有的item向量均通过embedding的方式映射到维度相同的空间中， 同一类型的item向量通过average整合成一个向量。

在训练阶段， 我们同样采取正负样本法， 去优化所有正样本的概率之和。目标方程如下：

![DSSM](/img/ZH15.jpg)

在应用阶段， 我们则通过id找到用户在该向量空间中的对应（item， user）向量， 并通过KNN等方式找到附近的（item， user向量）， 从而完成item召回。

更加具体的训练细节请自行查阅原论文。 

## Youtube DNN与Multi View DSSM的对比

由于Youtube DNN同样接受来自多个不同item view的用户数据， 同时， 并没有区分用户的长短期需求， 因此， 最适合比较的对象是MV-DSSM.

![DSSM](/img/ZH16.jpg)

可见， MV-DSSM最大的问题在于难于收敛和难于加入时序特征（即难于与TDSSM融合）， 由于我们目前的业务需求对短期需求变化的敏感度要求并不高， 所以我们目前还是主要考虑MVDSSM这条线。

MVDSSM在鲁棒性， scalability和冷启动方面的优势更命中我们业务中的实际痛点。

# 更多内容

* 下一节我们将介绍最基本的DNN排序模型。
* 在DSSM中我们简单提到了LTR排序模型， 将在第三节中介绍。 
* 最后， User和Item的特征选择及自动选择特征的FM系列模型， 将从第四节开始介绍。



